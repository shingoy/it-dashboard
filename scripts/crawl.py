"""
æ”¿åºœITä¼šè­°ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç©æ¥µåé›†ç‰ˆï¼‰
"""

import json
import os
import hashlib
import time
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

# è¨­å®š
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
CACHE_DIR = DATA_DIR / "cache"
OUTPUT_DIR = BASE_DIR / "public"
FAILED_LOG = DATA_DIR / "failed_urls.json"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
DATA_DIR.mkdir(exist_ok=True)
CACHE_DIR.mkdir(exist_ok=True)
OUTPUT_DIR.mkdir(exist_ok=True)

# ç©æ¥µçš„ãªåé›†è¨­å®š
MAX_DOCUMENTS_PER_RUN = int(os.getenv('MAX_DOCUMENTS', '100'))  # 10 â†’ 100ã«å¢—é‡
MAX_DOCUMENTS_PER_MEETING = int(os.getenv('MAX_PER_MEETING', '10'))  # 1 â†’ 10ã«å¢—é‡
DOWNLOAD_TIMEOUT = 30
REQUEST_TIMEOUT = 10

# ãƒ—ãƒªã‚»ãƒƒãƒˆä¼šè­°è¨­å®šï¼ˆæ¤œè¨¼æ¸ˆã¿ + æ–°è¦è¿½åŠ ï¼‰
PRESET_MEETINGS = {
    "digital_agency": {
        "name": "ãƒ‡ã‚¸ã‚¿ãƒ«åº",
        "meetings": [
            {
                "name": "ãƒ‡ã‚¸ã‚¿ãƒ«è‡¨æ™‚è¡Œæ”¿èª¿æŸ»ä¼š",
                "url": "https://www.digital.go.jp/councils/administrative-research",
                "type": "html_list"
            },
            {
                "name": "ãƒ‡ã‚¸ã‚¿ãƒ«ç¤¾ä¼šæ¨é€²ä¼šè­°",
                "url": "https://www.digital.go.jp/councils/social-promotion",
                "type": "html_list"
            },
            {
                "name": "ãƒ‡ãƒ¼ã‚¿æˆ¦ç•¥æ¨é€²ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ã‚°ãƒ«ãƒ¼ãƒ—",
                "url": "https://www.digital.go.jp/councils/data-strategy-wg",
                "type": "html_list"
            },
            {
                "name": "ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ¬ã‚¸ã‚¹ãƒˆãƒª",
                "url": "https://www.digital.go.jp/policies/base_registry",
                "type": "html_list"
            },
            # æ–°è¦è¿½åŠ : ãƒ‡ã‚¸ã‚¿ãƒ«åºã®ä¸»è¦ãƒšãƒ¼ã‚¸
            {
                "name": "ãƒ‡ã‚¸ã‚¿ãƒ«ç¤¾ä¼šæ¨é€²æ¨™æº–ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
                "url": "https://www.digital.go.jp/resources/standard_guidelines/",
                "type": "html_list"
            },
        ]
    },
    "cabinet_office": {
        "name": "å†…é–£åºœãƒ»å†…é–£å®˜æˆ¿",
        "meetings": [
            {
                "name": "AIæˆ¦ç•¥ä¼šè­°",
                "url": "https://www8.cao.go.jp/cstp/ai/",
                "type": "html_list"
            },
            {
                "name": "ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æˆ¦ç•¥æœ¬éƒ¨",
                "url": "https://www.nisc.go.jp/council/",
                "type": "html_list"
            },
            # æ–°è¦è¿½åŠ : çµ±åˆã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥æ¨é€²ä¼šè­°
            {
                "name": "çµ±åˆã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥",
                "url": "https://www8.cao.go.jp/cstp/togo/index.html",
                "type": "html_list"
            },
        ]
    },
    "mic": {
        "name": "ç·å‹™çœ",
        "meetings": [
            {
                "name": "ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ»ã‚¬ãƒãƒ¡ãƒ³ãƒˆ",
                "url": "https://www.soumu.go.jp/menu_seisaku/ictseisaku/ictriyou/",
                "type": "html_list"
            },
            # æ–°è¦è¿½åŠ : ICTæ”¿ç­–é–¢é€£
            {
                "name": "æƒ…å ±é€šä¿¡å¯©è­°ä¼š",
                "url": "https://www.soumu.go.jp/main_sosiki/joho_tsusin/policyreports/joho_tsusin/index.html",
                "type": "html_list"
            },
        ]
    },
    "meti": {
        "name": "çµŒæ¸ˆç”£æ¥­çœ",
        "meetings": [
            {
                "name": "ãƒ‡ã‚¸ã‚¿ãƒ«ç”£æ¥­ã®å‰µå‡ºã«å‘ã‘ãŸç ”ç©¶ä¼š",
                "url": "https://www.meti.go.jp/shingikai/mono_info_service/digital_sangyo/index.html",
                "type": "html_list"
            },
            # æ–°è¦è¿½åŠ : DXé–¢é€£
            {
                "name": "ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³",
                "url": "https://www.meti.go.jp/policy/it_policy/dx/index.html",
                "type": "html_list"
            },
        ]
    },
    "mhlw": {
        "name": "åšç”ŸåŠ´åƒçœ",
        "meetings": [
            {
                "name": "åŒ»ç™‚DXä»¤å’Œãƒ“ã‚¸ãƒ§ãƒ³2030",
                "url": "https://www.mhlw.go.jp/stf/newpage_28422.html",
                "type": "html_list"
            },
        ]
    },
    "mext": {
        "name": "æ–‡éƒ¨ç§‘å­¦çœ",
        "meetings": [
            {
                "name": "GIGAã‚¹ã‚¯ãƒ¼ãƒ«æ§‹æƒ³",
                "url": "https://www.mext.go.jp/a_menu/shotou/zyouhou/detail/mext_00002.html",
                "type": "html_list"
            },
        ]
    },
    "ppc": {
        "name": "å€‹äººæƒ…å ±ä¿è­·å§”å“¡ä¼š",
        "meetings": [
            {
                "name": "å€‹äººæƒ…å ±ä¿è­·å§”å“¡ä¼š",
                "url": "https://www.ppc.go.jp/aboutus/commission/",
                "type": "html_list"
            },
        ]
    },
    "other": {
        "name": "ãã®ä»–",
        "meetings": [
            {
                "name": "CIOé€£çµ¡ä¼šè­°",
                "url": "https://cio.go.jp/",
                "type": "html_list"
            },
        ]
    },
}

class MeetingCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Government IT Dashboard Crawler)'
        })
        self.failed_urls = []
        self.docs_cache = self.load_docs_cache()
        self.existing_docs = self.load_existing_docs()
        
    def load_docs_cache(self):
        """æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
        cache_file = DATA_DIR / "docs_cache.json"
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def load_existing_docs(self):
        """æ—¢å­˜ã®åé›†æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        output_file = DATA_DIR / "collected_docs.json"
        if output_file.exists():
            with open(output_file, 'r', encoding='utf-8') as f:
                docs = json.load(f)
                return {doc['id']: doc for doc in docs}
        return {}
    
    def save_docs_cache(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        cache_file = DATA_DIR / "docs_cache.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.docs_cache, f, ensure_ascii=False, indent=2)
    
    def fetch_url(self, url, retries=2):
        """URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                return response
            except Exception as e:
                if attempt == retries - 1:
                    # æœ€å¾Œã®è©¦è¡Œã§ã®ã¿ãƒ­ã‚°
                    print(f"  âš ï¸  Failed: {str(e)[:60]}")
                    self.failed_urls.append({
                        "url": url,
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    })
                    return None
                time.sleep(1)
        return None
    
    def parse_meeting_list(self, meeting_config, max_docs=None):
        """ä¼šè­°ãƒšãƒ¼ã‚¸ã‹ã‚‰è­°äº‹éŒ²ãƒ»è³‡æ–™ã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
        url = meeting_config['url']
        print(f"\nğŸ“‹ {meeting_config['name']}")
        
        response = self.fetch_url(url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        documents = []
        
        # PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
        pdf_links = soup.find_all('a', href=lambda x: x and x.endswith('.pdf'))
        
        if len(pdf_links) == 0:
            print(f"   No PDFs found")
            return []
        
        print(f"   Found {len(pdf_links)} PDFs")
        
        # æœ€å¤§æ•°ã‚’åˆ¶é™
        limit = max_docs if max_docs else MAX_DOCUMENTS_PER_MEETING
        new_count = 0
        
        for link in pdf_links:
            if new_count >= limit:
                break
                
            pdf_url = urljoin(url, link.get('href'))
            
            # æ—¢ã«åé›†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            url_hash = hashlib.md5(pdf_url.encode()).hexdigest()
            if url_hash in self.existing_docs or url_hash in self.docs_cache:
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = link.get_text(strip=True)
            if not title:
                parent = link.find_parent(['li', 'div', 'td'])
                if parent:
                    title = parent.get_text(strip=True)[:100]
            
            # æ—¥ä»˜ã‚’æ¨æ¸¬
            date_str = self.extract_date_from_element(link)
            
            doc = {
                "id": url_hash,
                "meeting": meeting_config['name'],
                "agency": meeting_config.get('agency', ''),
                "title": title,
                "url": pdf_url,
                "file_type": "pdf",
                "date": date_str,
                "collected_at": datetime.now().isoformat()
            }
            
            documents.append(doc)
            new_count += 1
        
        if new_count > 0:
            print(f"   âœ“ {new_count} new documents")
        
        return documents
    
    def extract_date_from_element(self, element):
        """è¦ç´ å‘¨è¾ºã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
        import re
        
        parent = element.find_parent(['li', 'div', 'td', 'tr'])
        if parent:
            text = parent.get_text()
            
            # ä»¤å’ŒXå¹´YæœˆZæ—¥
            match = re.search(r'ä»¤å’Œ(\d+)å¹´(\d+)æœˆ(\d+)æ—¥', text)
            if match:
                year = 2018 + int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                return f"{year}-{month:02d}-{day:02d}"
            
            # YYYYå¹´MMæœˆDDæ—¥
            match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
            if match:
                return f"{match.group(1)}-{int(match.group(2)):02d}-{int(match.group(3)):02d}"
            
            # YYYY/MM/DD or YYYY-MM-DD
            match = re.search(r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})', text)
            if match:
                return f"{match.group(1)}-{int(match.group(2)):02d}-{int(match.group(3)):02d}"
        
        return datetime.now().strftime('%Y-%m-%d')
    
    def download_pdf(self, doc):
        """PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰"""
        pdf_path = CACHE_DIR / f"{doc['id']}.pdf"
        
        # æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if pdf_path.exists():
            return str(pdf_path)
        
        try:
            response = self.session.get(doc['url'], timeout=DOWNLOAD_TIMEOUT, stream=True)
            response.raise_for_status()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆ20MBä»¥ä¸Šã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > 20 * 1024 * 1024:
                print(f"  âš ï¸  Too large (>20MB): {doc['title'][:40]}")
                return None
            
            with open(pdf_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return str(pdf_path)
            
        except Exception as e:
            print(f"  âŒ Download failed: {doc['title'][:30]}")
            self.failed_urls.append({
                "url": doc['url'],
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            return None
    
    def crawl_all(self):
        """å…¨ä¼šè­°ã‚’å·¡å›ï¼ˆç©æ¥µåé›†ï¼‰"""
        all_new_documents = []
        download_count = 0
        
        start_time = time.time()
        max_runtime = 4 * 60  # 4åˆ†ã§å¼·åˆ¶çµ‚äº†
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ Aggressive Crawling Strategy")
        print(f"   Max per meeting: {MAX_DOCUMENTS_PER_MEETING}")
        print(f"   Total limit: {MAX_DOCUMENTS_PER_RUN}")
        print(f"   Already collected: {len(self.existing_docs)}")
        print(f"{'='*60}")
        
        for agency_id, agency_config in PRESET_MEETINGS.items():
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            if time.time() - start_time > max_runtime:
                print(f"\nâ±ï¸  Runtime limit reached, stopping")
                break
            
            print(f"\nğŸ›ï¸  {agency_config['name']}")
            
            for meeting in agency_config['meetings']:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
                if time.time() - start_time > max_runtime:
                    break
                
                # å…¨ä½“ã®æœ€å¤§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°ãƒã‚§ãƒƒã‚¯
                if download_count >= MAX_DOCUMENTS_PER_RUN:
                    print(f"\nâš ï¸  Reached run limit ({MAX_DOCUMENTS_PER_RUN})")
                    break
                
                meeting['agency'] = agency_config['name']
                documents = self.parse_meeting_list(meeting)
                
                # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                for doc in documents:
                    if download_count >= MAX_DOCUMENTS_PER_RUN:
                        break
                    
                    pdf_path = self.download_pdf(doc)
                    if pdf_path:
                        doc['pdf_path'] = pdf_path
                        all_new_documents.append(doc)
                        self.docs_cache[doc['id']] = doc
                        download_count += 1
                        
                        if download_count % 10 == 0:
                            print(f"\nğŸ“Š Progress: {download_count}/{MAX_DOCUMENTS_PER_RUN}")
                
                time.sleep(0.3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        elapsed = time.time() - start_time
        print(f"\n{'='*60}")
        print(f"â±ï¸  Time: {elapsed:.1f}s")
        print(f"ğŸ“š New documents: {len(all_new_documents)}")
        print(f"ğŸ“š Total documents: {len(self.existing_docs) + len(all_new_documents)}")
        
        return all_new_documents
    
    def save_results(self, new_documents):
        """åé›†çµæœã‚’ä¿å­˜ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ï¼‰"""
        # æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨æ–°è¦ã‚’çµ±åˆ
        all_documents = list(self.existing_docs.values()) + new_documents
        
        output_file = DATA_DIR / "collected_docs.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_documents, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Saved {len(all_documents)} total documents")
        print(f"   ({len(new_documents)} new + {len(self.existing_docs)} existing)")
        
        if self.failed_urls:
            with open(FAILED_LOG, 'w', encoding='utf-8') as f:
                json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
            print(f"âš ï¸  {len(self.failed_urls)} failed URLs")
        
        self.save_docs_cache()

def main():
    print("ğŸš€ Government IT Document Crawler (Aggressive Mode)")
    print(f"   Max per run: {MAX_DOCUMENTS_PER_RUN}")
    print(f"   Max per meeting: {MAX_DOCUMENTS_PER_MEETING}")
    
    crawler = MeetingCrawler()
    new_documents = crawler.crawl_all()
    crawler.save_results(new_documents)
    
    print("\n" + "="*60)
    print(f"âœ¨ Crawling complete!")
    print("="*60)

if __name__ == "__main__":
    main()