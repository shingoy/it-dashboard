"""
æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
BM25ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒˆãƒ¬ãƒ³ãƒ‰é›†è¨ˆJSONã‚’ç”Ÿæˆ
"""

import json
import math
import re
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime
from typing import List, Dict

BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
EXTRACTED_DIR = DATA_DIR / "extracted"
PUBLIC_DIR = BASE_DIR / "public"
INDEX_DIR = PUBLIC_DIR / "index-shards"
TRENDS_DIR = PUBLIC_DIR / "trends"

INDEX_DIR.mkdir(parents=True, exist_ok=True)
TRENDS_DIR.mkdir(parents=True, exist_ok=True)

class IndexBuilder:
    def __init__(self):
        self.all_chunks = []
        self.all_docs = []
        self.idf_cache = {}
        
    def tokenize(self, text: str) -> List[str]:
        """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # 2-4æ–‡å­—ã®æ—¥æœ¬èªå˜èªã‚’æŠ½å‡º
        tokens = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,4}', text)
        # è‹±æ•°å­—ã‚‚æŠ½å‡º
        tokens.extend(re.findall(r'[A-Za-z0-9]+', text))
        return [t.lower() for t in tokens]
    
    def calculate_bm25_scores(self, chunks: List[Dict]) -> List[Dict]:
        """BM25ã‚¹ã‚³ã‚¢è¨ˆç®—ã®ãŸã‚ã®çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ """
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        k1 = 1.5
        b = 0.75
        
        # å…¨ãƒãƒ£ãƒ³ã‚¯ã®å¹³å‡æ–‡å­—æ•°
        avg_length = sum(c['char_count'] for c in chunks) / len(chunks) if chunks else 1
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
        for chunk in chunks:
            chunk['tokens'] = self.tokenize(chunk['text'])
            chunk['token_count'] = len(chunk['tokens'])
        
        # IDFè¨ˆç®—
        total_docs = len(chunks)
        doc_freq = defaultdict(int)
        
        for chunk in chunks:
            unique_tokens = set(chunk['tokens'])
            for token in unique_tokens:
                doc_freq[token] += 1
        
        # IDFå€¤ã‚’è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        for token, df in doc_freq.items():
            idf = math.log((total_docs - df + 0.5) / (df + 0.5) + 1)
            self.idf_cache[token] = idf
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã®BM25ç”¨çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        for chunk in chunks:
            chunk['avg_length'] = avg_length
            chunk['k1'] = k1
            chunk['b'] = b
        
        return chunks
    
    def create_shards(self, chunks: List[Dict], shard_size: int = 100):
        """ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚·ãƒ£ãƒ¼ãƒ‰ã«åˆ†å‰²"""
        # ä¼šè­°Ã—æœˆã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        groups = defaultdict(list)
        
        for chunk in chunks:
            # YYYY-MMå½¢å¼
            month_key = chunk['date'][:7] if chunk.get('date') else '2025-01'
            meeting = chunk.get('meeting', 'unknown')
            key = f"{meeting}_{month_key}"
            groups[key].append(chunk)
        
        # ã‚·ãƒ£ãƒ¼ãƒ‰ç”Ÿæˆ
        shards = []
        for group_key, group_chunks in groups.items():
            # ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¤§ãã„å ´åˆã¯åˆ†å‰²
            for i in range(0, len(group_chunks), shard_size):
                shard_chunks = group_chunks[i:i+shard_size]
                
                # è»½é‡åŒ–: æ¤œç´¢ã«ä¸è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é™¤å¤–
                lightweight_chunks = []
                for chunk in shard_chunks:
                    lightweight_chunks.append({
                        'chunk_id': chunk['chunk_id'],
                        'doc_id': chunk['doc_id'],
                        'text': chunk['text'][:500],  # ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”¨ã«çŸ­ç¸®
                        'full_text': chunk['text'],  # æ¤œç´¢ç”¨
                        'tokens': chunk['tokens'][:100],  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™
                        'meeting': chunk['meeting'],
                        'agency': chunk['agency'],
                        'title': chunk['title'],
                        'date': chunk['date'],
                        'url': chunk['url'],
                        'page_from': chunk['page_from'],
                        'page_to': chunk['page_to'],
                        'char_count': chunk['char_count']
                    })
                
                shard = {
                    'shard_id': f"{group_key}_{i//shard_size}",
                    'group': group_key,
                    'chunk_count': len(shard_chunks),
                    'chunks': lightweight_chunks,
                    'idf': self.idf_cache  # IDFæƒ…å ±ã‚’å«ã‚ã‚‹
                }
                
                shards.append(shard)
        
        return shards
    
    def save_shards(self, shards: List[Dict]):
        """ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        shard_index = []
        
        for shard in shards:
            shard_id = shard['shard_id']
            filename = f"{shard_id}.json"
            filepath = INDEX_DIR / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(shard, f, ensure_ascii=False, separators=(',', ':'))
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            shard_index.append({
                'shard_id': shard_id,
                'filename': filename,
                'group': shard['group'],
                'chunk_count': shard['chunk_count']
            })
            
            print(f"  âœ“ Saved shard: {filename} ({shard['chunk_count']} chunks)")
        
        # ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
        index_file = INDEX_DIR / "_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(shard_index, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Saved {len(shards)} shards to {INDEX_DIR}")
    
    def create_docs_meta(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        docs_meta = []
        
        for doc in self.all_docs:
            docs_meta.append({
                'doc_id': doc['doc_id'],
                'meeting': doc['metadata']['meeting'],
                'agency': doc['metadata']['agency'],
                'title': doc['metadata']['title'],
                'date': doc['metadata']['date'],
                'url': doc['metadata']['url'],
                'pages': doc['pages'],
                'chunks_count': len(doc['chunks'])
            })
        
        meta_file = PUBLIC_DIR / "docs-meta.json"
        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(docs_meta, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Saved docs metadata: {meta_file}")
    
    def generate_trends(self):
        """æœˆæ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰é›†è¨ˆ"""
        # æœˆã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        monthly_data = defaultdict(lambda: {
            'keywords': Counter(),
            'meetings': Counter(),
            'docs': []
        })
        
        for chunk in self.all_chunks:
            month = chunk['date'][:7] if chunk.get('date') else '2025-01'
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é›†è¨ˆ
            for token in chunk.get('tokens', [])[:50]:
                monthly_data[month]['keywords'][token] += 1
            
            # ä¼šè­°åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
            meeting = chunk.get('meeting', 'unknown')
            monthly_data[month]['meetings'][meeting] += 1
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¨˜éŒ²
            if chunk['doc_id'] not in [d['doc_id'] for d in monthly_data[month]['docs']]:
                monthly_data[month]['docs'].append({
                    'doc_id': chunk['doc_id'],
                    'title': chunk['title'],
                    'meeting': chunk['meeting']
                })
        
        # æœˆã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        for month, data in monthly_data.items():
            # ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            top_keywords = [
                {'term': term, 'count': count}
                for term, count in data['keywords'].most_common(50)
            ]
            
            # ä¼šè­°åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
            meeting_counts = [
                {'meeting': meeting, 'count': count}
                for meeting, count in data['meetings'].most_common()
            ]
            
            trend_data = {
                'month': month,
                'keywords': top_keywords,
                'meetings': meeting_counts,
                'doc_count': len(data['docs']),
                'chunk_count': sum(data['meetings'].values())
            }
            
            trend_file = TRENDS_DIR / f"{month}.json"
            with open(trend_file, 'w', encoding='utf-8') as f:
                json.dump(trend_data, f, ensure_ascii=False, indent=2)
            
            print(f"  âœ“ Generated trend: {month}")
        
        print(f"\nâœ… Generated {len(monthly_data)} monthly trends")
    
    def load_all_extractions(self):
        """å…¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        extraction_files = list(EXTRACTED_DIR.glob("*.json"))
        
        if not extraction_files:
            print("âŒ No extracted files found. Run extract.py first.")
            return False
        
        print(f"ğŸ“š Loading {len(extraction_files)} extracted documents...")
        
        for file in extraction_files:
            with open(file, 'r', encoding='utf-8') as f:
                doc_data = json.load(f)
                self.all_docs.append(doc_data)
                self.all_chunks.extend(doc_data['chunks'])
        
        print(f"   Loaded {len(self.all_docs)} documents")
        print(f"   Loaded {len(self.all_chunks)} chunks")
        
        return True
    
    def build(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ“ãƒ«ãƒ‰"""
        print("ğŸš€ Building search index")
        print("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.load_all_extractions():
            return
        
        # BM25çµ±è¨ˆè¨ˆç®—
        print("\nğŸ“Š Calculating BM25 statistics...")
        self.all_chunks = self.calculate_bm25_scores(self.all_chunks)
        
        # ã‚·ãƒ£ãƒ¼ãƒ‰ä½œæˆ
        print("\nğŸ”¨ Creating shards...")
        shards = self.create_shards(self.all_chunks)
        self.save_shards(shards)
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        print("\nğŸ“‹ Creating document metadata...")
        self.create_docs_meta()
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰é›†è¨ˆ
        print("\nğŸ“ˆ Generating trends...")
        self.generate_trends()
        
        print("\n" + "="*60)
        print("âœ¨ Index build complete!")
        print(f"   Total chunks: {len(self.all_chunks)}")
        print(f"   Total shards: {len(shards)}")
        print(f"   Output directory: {PUBLIC_DIR}")
        print("="*60)

def main():
    builder = IndexBuilder()
    builder.build()

if __name__ == "__main__":
    main()
