"""
PDF/HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆå¢—åˆ†å‡¦ç†ç‰ˆãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰
"""

import json
import re
import sys
from pathlib import Path
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import time
import gc

# æ¨™æº–å‡ºåŠ›ã®ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼ˆå³åº§ã«å‡ºåŠ›ï¼‰
sys.stdout.reconfigure(line_buffering=True)

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    print("âš ï¸  PyMuPDF not available", flush=True)
    PYMUPDF_AVAILABLE = False

BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
CACHE_DIR = DATA_DIR / "cache"
EXTRACTED_DIR = DATA_DIR / "extracted"

EXTRACTED_DIR.mkdir(exist_ok=True)

# ç’°å¢ƒå¤‰æ•°ã§ä¸¦åˆ—æ•°ã‚’åˆ¶å¾¡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4ã«å‰Šæ¸›ï¼‰
MAX_WORKERS = int(os.environ.get('EXTRACT_MAX_WORKERS', '4'))
# 1å›ã®å®Ÿè¡Œã§å‡¦ç†ã™ã‚‹PDFã®æœ€å¤§æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ20ã«å‰Šæ¸›ï¼‰
BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '20'))
# å€‹åˆ¥PDFã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ180ç§’=3åˆ†ã«çŸ­ç¸®ï¼‰
PDF_TIMEOUT = int(os.environ.get('PDF_TIMEOUT', '180'))

class TextExtractor:
    def __init__(self):
        self.chunk_size = 1200
        self.chunk_overlap = 200
    
    def extract_from_pdf(self, pdf_path: str, doc_id: str, max_pages: int = 100) -> Dict:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰"""
        if not PYMUPDF_AVAILABLE:
            return {
                "success": False,
                "error": "PyMuPDF not installed"
            }
        
        doc = None
        try:
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            
            # ãƒšãƒ¼ã‚¸æ•°ãŒå¤šã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if total_pages > max_pages:
                return {
                    "success": False,
                    "error": f"PDF too large: {total_pages} pages (max: {max_pages})",
                    "skipped": True
                }
            
            pages = []
            
            # ãƒšãƒ¼ã‚¸ã”ã¨ã«å‡¦ç†ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰
            for page_num in range(total_pages):
                # 10ãƒšãƒ¼ã‚¸ã”ã¨ã«é€²æ—ã‚’è¡¨ç¤º
                if page_num > 0 and page_num % 10 == 0:
                    print(f"    ğŸ“– Page {page_num}/{total_pages} ({doc_id})", flush=True)
                
                page = doc[page_num]
                text = page.get_text()
                text = self.clean_text(text)
                
                if text.strip():
                    pages.append({
                        "page_num": page_num + 1,
                        "text": text,
                        "char_count": len(text)
                    })
                
                del page
            
            result = {
                "success": True,
                "pages": pages,
                "total_pages": len(pages),
                "total_chars": sum(p['char_count'] for p in pages)
            }
            
            return result
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
        finally:
            if doc is not None:
                doc.close()
                del doc
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f]', '', text)
        return text.strip()
    
    def create_chunks(self, pages: List[Dict], doc_id: str, metadata: Dict) -> List[Dict]:
        """ãƒšãƒ¼ã‚¸ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        print(f"    ğŸ”ª Creating chunks for {doc_id}...", flush=True)
        
        chunks = []
        chunk_index = 0
        
        full_text = "\n\n".join([p['text'] for p in pages])
        
        page_boundaries = []
        current_pos = 0
        for page in pages:
            page_text = page['text']
            page_boundaries.append({
                'page_num': page['page_num'],
                'start': current_pos,
                'end': current_pos + len(page_text)
            })
            current_pos += len(page_text) + 2
        
        start = 0
        while start < len(full_text):
            end = start + self.chunk_size
            
            if end < len(full_text):
                period_pos = full_text.rfind('ã€‚', start, end + 100)
                if period_pos > start:
                    end = period_pos + 1
            
            chunk_text = full_text[start:end].strip()
            
            if chunk_text:
                page_from, page_to = self.get_page_range(start, end, page_boundaries)
                
                chunks.append({
                    "chunk_id": f"{doc_id}_c{chunk_index}",
                    "doc_id": doc_id,
                    "chunk_index": chunk_index,
                    "text": chunk_text,
                    "page_from": page_from,
                    "page_to": page_to,
                    "char_count": len(chunk_text),
                    "position": start,
                    **metadata
                })
                
                chunk_index += 1
            
            start = end - self.chunk_overlap
            if start >= len(full_text):
                break
        
        return chunks
    
    def get_page_range(self, start: int, end: int, page_boundaries: List[Dict]) -> tuple:
        """ãƒãƒ£ãƒ³ã‚¯ã®é–‹å§‹ãƒ»çµ‚äº†ä½ç½®ã‹ã‚‰è©²å½“ãƒšãƒ¼ã‚¸ç¯„å›²ã‚’å–å¾—"""
        page_from = None
        page_to = None
        
        for boundary in page_boundaries:
            if page_from is None and start >= boundary['start'] and start < boundary['end']:
                page_from = boundary['page_num']
            if end >= boundary['start'] and end <= boundary['end']:
                page_to = boundary['page_num']
        
        if page_from is None:
            page_from = page_boundaries[0]['page_num'] if page_boundaries else 1
        if page_to is None:
            page_to = page_boundaries[-1]['page_num'] if page_boundaries else 1
        
        return page_from, page_to
    
    def extract_keywords(self, text: str, top_n: int = 10) -> List[Dict]:
        """ç°¡æ˜“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        print(f"    ğŸ”‘ Extracting keywords...", flush=True)
        
        stop_words = set(['ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã‚ˆã†', 'ã“ã‚Œ', 'ãã‚Œ', 'ãªã©', 'ã«ã¤ã„ã¦', 'ã«ãŠã‘ã‚‹'])
        words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,4}', text)
        
        word_freq = {}
        for word in words:
            if word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
        return [{"term": word, "count": count} for word, count in sorted_words]
    
    def is_already_processed(self, doc_id: str) -> bool:
        """æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        output_file = EXTRACTED_DIR / f"{doc_id}.json"
        return output_file.exists()
    
    def process_document_with_timeout(self, doc: Dict, index: int, total: int) -> Dict:
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†"""
        doc_id = doc['id']
        
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if self.is_already_processed(doc_id):
            print(f"  [{index}/{total}] â­ï¸  Already processed: {doc_id}", flush=True)
            return {
                "doc_id": doc_id,
                "success": True,
                "already_processed": True
            }
        
        try:
            return self.process_document(doc, index, total)
        except Exception as e:
            print(f"  [{index}/{total}] âŒ Exception: {str(e)[:100]}", flush=True)
            return {
                "doc_id": doc_id,
                "success": False,
                "error": str(e)
            }
    
    def process_document(self, doc: Dict, index: int, total: int) -> Dict:
        """1ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆè©³ç´°ãªé€²æ—è¡¨ç¤ºä»˜ãï¼‰"""
        doc_id = doc['id']
        pdf_path = doc.get('pdf_path')
        
        if not pdf_path or not Path(pdf_path).exists():
            print(f"  [{index}/{total}] âŒ {doc_id}: PDF file not found", flush=True)
            return {
                "doc_id": doc_id,
                "success": False,
                "error": "PDF file not found"
            }
        
        file_size_mb = Path(pdf_path).stat().st_size / (1024 * 1024)
        print(f"\n  [{index}/{total}] ğŸ“„ Processing: {doc['title'][:50]}...", flush=True)
        print(f"    Size: {file_size_mb:.1f}MB | ID: {doc_id}", flush=True)
        
        start_time = time.time()
        
        # PDFæŠ½å‡ºï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰
        print(f"    ğŸ” Extracting text from PDF...", flush=True)
        extraction = self.extract_from_pdf(pdf_path, doc_id)
        
        if not extraction['success']:
            print(f"  [{index}/{total}] âŒ Extraction failed: {extraction.get('error', 'Unknown')}", flush=True)
            return {
                "doc_id": doc_id,
                "success": False,
                "error": extraction.get('error')
            }
        
        print(f"    âœ“ Extracted {extraction['total_pages']} pages, {extraction['total_chars']:,} chars", flush=True)
        
        metadata = {
            "meeting": doc['meeting'],
            "agency": doc['agency'],
            "title": doc['title'],
            "date": doc['date'],
            "url": doc['url']
        }
        
        # ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        chunks = self.create_chunks(extraction['pages'], doc_id, metadata)
        print(f"    âœ“ Created {len(chunks)} chunks", flush=True)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        full_text = "\n".join([p['text'] for p in extraction['pages']])
        keywords = self.extract_keywords(full_text)
        print(f"    âœ“ Extracted {len(keywords)} keywords", flush=True)
        
        # ä¿å­˜
        print(f"    ğŸ’¾ Saving to file...", flush=True)
        output = {
            "doc_id": doc_id,
            "metadata": metadata,
            "pages": len(extraction['pages']),
            "chunks": chunks,
            "keywords": keywords
        }
        
        output_file = EXTRACTED_DIR / f"{doc_id}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        elapsed = time.time() - start_time
        print(f"  [{index}/{total}] âœ… Done in {elapsed:.1f}s | {len(chunks)} chunks", flush=True)
        
        return {
            "doc_id": doc_id,
            "success": True,
            "chunks_count": len(chunks),
            "keywords_count": len(keywords),
            "processing_time": elapsed
        }
    
    def process_all(self):
        """å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å¢—åˆ†å‡¦ç†"""
        docs_file = DATA_DIR / "collected_docs.json"
        if not docs_file.exists():
            print("âŒ No collected documents found", flush=True)
            return
        
        with open(docs_file, 'r', encoding='utf-8') as f:
            all_documents = json.load(f)
        
        # æœªå‡¦ç†ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’æŠ½å‡º
        unprocessed_docs = [
            doc for doc in all_documents 
            if not self.is_already_processed(doc['id'])
        ]
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆ¶é™
        documents = unprocessed_docs[:BATCH_SIZE]
        
        total = len(documents)
        total_all = len(all_documents)
        already_processed = len(all_documents) - len(unprocessed_docs)
        
        print(f"ğŸ“Š Status:", flush=True)
        print(f"   Total documents: {total_all}", flush=True)
        print(f"   Already processed: {already_processed}", flush=True)
        print(f"   Remaining: {len(unprocessed_docs)}", flush=True)
        print(f"   Processing this batch: {total}", flush=True)
        print(f"   Workers: {MAX_WORKERS}", flush=True)
        print("", flush=True)
        
        if total == 0:
            print("âœ… All documents already processed!", flush=True)
            return
        
        results = []
        
        if MAX_WORKERS == 1:
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†
            print("ğŸ“Œ Running in sequential mode", flush=True)
            for i, doc in enumerate(documents):
                result = self.process_document_with_timeout(doc, i+1, total)
                results.append(result)
        else:
            # ä¸¦åˆ—å‡¦ç†
            print(f"ğŸ“Œ Running in parallel mode ({MAX_WORKERS} workers)", flush=True)
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_doc = {}
                
                for i, doc in enumerate(documents):
                    future = executor.submit(self.process_document_with_timeout, doc, i+1, total)
                    future_to_doc[future] = doc
                
                for future in as_completed(future_to_doc):
                    try:
                        result = future.result(timeout=PDF_TIMEOUT)
                        results.append(result)
                    except TimeoutError:
                        doc = future_to_doc[future]
                        print(f"  â±ï¸  TIMEOUT: {doc['title'][:50]}", flush=True)
                        results.append({
                            "doc_id": doc['id'],
                            "success": False,
                            "error": "Processing timeout",
                            "timeout": True
                        })
                    except Exception as e:
                        doc = future_to_doc[future]
                        print(f"  âŒ Exception: {doc['id']}: {e}", flush=True)
                        results.append({
                            "doc_id": doc['id'],
                            "success": False,
                            "error": str(e)
                        })
        
        summary = {
            "total_documents": total_all,
            "already_processed": already_processed,
            "batch_size": total,
            "successful": sum(1 for r in results if r['success'] and not r.get('already_processed')),
            "already_processed_in_batch": sum(1 for r in results if r.get('already_processed')),
            "failed": sum(1 for r in results if not r['success']),
            "timeout": sum(1 for r in results if r.get('timeout')),
            "remaining": len(unprocessed_docs) - total,
            "results": results
        }
        
        summary_file = DATA_DIR / "extraction_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*60}", flush=True)
        print(f"âœ… Batch extraction complete!", flush=True)
        print(f"   Processed: {summary['successful']}", flush=True)
        print(f"   Failed: {summary['failed']}", flush=True)
        print(f"   Timeout: {summary['timeout']}", flush=True)
        print(f"   Remaining: {summary['remaining']}", flush=True)
        completed = already_processed + summary['successful']
        progress = (completed / total_all * 100) if total_all > 0 else 0
        print(f"   Progress: {completed}/{total_all} ({progress:.1f}%)", flush=True)

def main():
    import datetime
    start_time = datetime.datetime.now()
    
    print("ğŸš€ Starting incremental text extraction", flush=True)
    print(f"â° Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
    print("="*60, flush=True)
    print(f"   Max workers: {MAX_WORKERS}", flush=True)
    print(f"   Batch size: {BATCH_SIZE}", flush=True)
    print(f"   PDF timeout: {PDF_TIMEOUT}s", flush=True)
    print("="*60, flush=True)
    
    extractor = TextExtractor()
    extractor.process_all()
    
    end_time = datetime.datetime.now()
    elapsed = (end_time - start_time).total_seconds()
    
    print("="*60, flush=True)
    print(f"âœ… Total time: {elapsed:.1f}s ({elapsed/60:.1f}min)", flush=True)
    print(f"â° End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)

if __name__ == "__main__":
    main()