"""
collected_docs.json ã® pdf_path ã‚’ä¿®å¾©ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
from pathlib import Path

BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
CACHE_DIR = DATA_DIR / "cache"

def fix_pdf_paths():
    docs_file = DATA_DIR / "collected_docs.json"
    
    if not docs_file.exists():
        print("âŒ collected_docs.json not found")
        return
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
    with open(docs_file, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    
    print(f"ğŸ“š Found {len(documents)} documents")
    
    fixed_count = 0
    missing_count = 0
    
    for doc in documents:
        doc_id = doc['id']
        cache_path = CACHE_DIR / f"{doc_id}.pdf"
        
        # pdf_pathãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not doc.get('pdf_path') or not Path(doc['pdf_path']).exists():
            if cache_path.exists():
                doc['pdf_path'] = str(cache_path)
                fixed_count += 1
                print(f"  âœ“ Fixed: {doc_id} -> {doc['title'][:40]}")
            else:
                missing_count += 1
                print(f"  âŒ Missing: {doc_id} -> {doc['title'][:40]}")
    
    # ä¿å­˜
    with open(docs_file, 'w', encoding='utf-8') as f:
        json.dump(documents, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*60}")
    print(f"âœ… Fixed {fixed_count} document paths")
    print(f"âš ï¸  {missing_count} PDFs still missing (need to re-download)")
    print(f"{'='*60}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚‹PDFã®æ•°ã‚’ç¢ºèª
    cached_pdfs = list(CACHE_DIR.glob("*.pdf"))
    print(f"\nğŸ“¦ Cache directory has {len(cached_pdfs)} PDFs")
    print(f"ğŸ“„ Documents with valid paths: {fixed_count}")

if __name__ == "__main__":
    fix_pdf_paths()